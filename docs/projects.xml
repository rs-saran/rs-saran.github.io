<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>RS Saran</title>
<link>https://rs-saran.github.io/projects.html</link>
<atom:link href="https://rs-saran.github.io/projects.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Sat, 03 May 2025 18:30:00 GMT</lastBuildDate>
<item>
  <title>Sakha: Building a Chatbot That Cares</title>
  <link>https://rs-saran.github.io/projects/20250122_serene_solace_sakha/</link>
  <description><![CDATA[ 





<p><a href="https://github.com/rs-saran/serene-solace-sakha" class="btn" target="_blank"><i class="fa-brands fa-github" aria-label="github"></i> source code</a> <!-- [ demo chat](https://github.com/rs-saran/serene-solace-sophy/blob/main/example_chat.txt){.btn target=_blank} --> <br></p>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>In today’s fast-paced world, many of us silently wrestle with stress, loneliness, and emotional overwhelm. Therapy is a powerful avenue for healing, but not everyone needs—or can access—that level of care. Sometimes, all it takes is a gentle check-in, a thoughtful nudge, or the simple comfort of feeling seen.</p>
<p>That’s where Sakha began—not as a replacement for professional help, but as a small, caring presence always within reach. Not as a replacement for professional help, but as a small, caring presence—always there, always listening. A friend in your pocket, ready to ask how you’re doing, suggest something that might lift your mood, or simply sit with you in a moment of stillness.</p>
<p>The vision was simple yet ambitious: create a chatbot that doesn’t just respond, but genuinely cares. One that offers warmth, understanding, and personalized encouragement—without judgment, pressure, or pretense.</p>
<p>The name Sakha, meaning “friend” in Sanskrit, wasn’t chosen lightly. It reflected the heart of what I wanted this project to embody: a companion who listens, nudges, and supports without judgment. Someone who remembers that you enjoy walks when you’re anxious, or that music lifts your mood when you’re feeling off.</p>
<p><strong>Scope of the Project</strong></p>
<p>Sakha isn’t trying to replace a human friend. It doesn’t claim to have all the answers or the emotional depth of years-long relationships. But what it <em>does</em> aim to be is something quite intentional: a steady, caring companion that’s always available—especially in moments when reaching out to a real person feels hard.</p>
<p>Instead of aiming for general-purpose conversation or entertainment, Sakha focuses on a specific emotional use-case:<br>
&gt; Supporting users in their day-to-day mental well-being through gentle check-ins, helpful activity nudges, and empathetic conversation.</p>
</section>
<section id="heres-what-sakha-is-designed-to-do-well" class="level3">
<h3 class="anchored" data-anchor-id="heres-what-sakha-is-designed-to-do-well">Here’s what Sakha is designed to do well:</h3>
<ul>
<li><strong>Recognize emotional states</strong> and adapt its responses accordingly.</li>
<li><strong>Suggest meaningful actions</strong> like personalized self-care activities, not generic tips.</li>
<li><strong>Hold continuity</strong> across a session and maintain contextual memory, enabling more nuanced and human-like responses.</li>
<li><strong>Handle sensitive situations</strong> with care, offering support resources rather than pretending to be a therapist.</li>
<li><strong>Balance modularity and empathy</strong>—using structured flows powered by LangGraph, but never sounding robotic.</li>
</ul>
<p>It’s a limited scope, yes, but it’s a foundation—one that can evolve over time as we explore more complex interactions, while keeping things simple and meaningful in the beginning.</p>
<hr>
</section>
<section id="building-sakha-one-decision-at-a-time" class="level2">
<h2 class="anchored" data-anchor-id="building-sakha-one-decision-at-a-time">Building Sakha: One Decision at a Time</h2>
<section id="where-do-you-even-start" class="level3">
<h3 class="anchored" data-anchor-id="where-do-you-even-start"><strong>Where Do You Even Start?</strong></h3>
<p>I didn’t set out with a 100-step master plan. Like most projects, Sakha started with a question: <em>How do I make a chatbot feel like a friend—not just in tone, but in behavior?</em> I knew I needed something flexible, expressive, and capable of holding a conversation in a way that didn’t feel robotic. That naturally pointed me toward LLMs.</p>
</section>
<section id="choosing-the-mind-llms-and-how-i-picked-mine" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-mind-llms-and-how-i-picked-mine"><strong>Choosing the Mind – LLMs and How I Picked Mine</strong></h3>
<p>With dozens of options, the choice wasn’t easy. GPT-4? Claude? LLaMA? Each model had tradeoffs in terms of access, pricing, and openness. Eventually, I chose to use open-source LLMs hosted through <strong>Groq’s API</strong>. Why?</p>
<ul>
<li><strong>Speed</strong>: Groq’s inference speeds were impressive.</li>
<li><strong>Flexibility</strong>: It supports open-source models like LLaMA3.3, which means I’m not locked into one ecosystem.</li>
<li><strong>Future-ready</strong>: I can always self-host these models later if needed, thanks to their open-source nature.</li>
</ul>
<p>I wrapped this through <strong>LangChain</strong>, which makes it easy to swap out models during experimentation and build proofs-of-concept quickly without worrying about underlying LLM plumbing.</p>
<hr>
</section>
<section id="langgraph-giving-structure-to-conversations" class="level3">
<h3 class="anchored" data-anchor-id="langgraph-giving-structure-to-conversations"><strong>LangGraph – Giving Structure to Conversations</strong></h3>
<p>Once I picked the brain, I needed to figure out how to guide it. Not every conversation should be flat and linear—some needed state, branching logic, and even tool use.</p>
<p>That’s where <strong>LangGraph</strong> came in.</p>
<ul>
<li>It lets me create multi-step workflows.</li>
<li>I can embed conditional logic into conversations (e.g., “Is the user in distress?” → switch flow).</li>
<li>It gives me conversational memory for continuity.</li>
<li>And it integrates smoothly with external tools and APIs.</li>
</ul>
<p>LangGraph powers Sakha’s <strong>conversation graph</strong>—the engine that routes how chats progress depending on context and user state.</p>
<hr>
</section>
<section id="flask-powering-interaction-with-the-bot" class="level3">
<h3 class="anchored" data-anchor-id="flask-powering-interaction-with-the-bot"><strong>Flask – Powering Interaction with the Bot</strong></h3>
<p>I used <strong>Flask</strong> not just to power Sakha’s simple UI but also to serve the core APIs that make chatting with the bot possible.</p>
<p>On the frontend, I kept things minimal—basic HTML and JavaScript for quick interactions. But behind the scenes, Flask handled the important parts: receiving user input, sending it to the LLM pipeline, and returning thoughtful responses.</p>
<p>It was the ideal choice because: - It’s lightweight and easy to set up. - It integrates seamlessly with Python, which is what the rest of Sakha is built on. - It made building both a simple web UI and a chat API straightforward and fast.</p>
<p>In short, Flask gave me just enough without getting in the way—letting me focus on what mattered most: the experience of the conversation.</p>
<hr>
</section>
</section>
<section id="under-the-hood-core-components-that-make-sakha-work" class="level2">
<h2 class="anchored" data-anchor-id="under-the-hood-core-components-that-make-sakha-work">Under the Hood: Core Components That Make Sakha Work</h2>
<p>At its heart, Sakha is more than just a chatbot—it’s a thoughtfully layered system designed to feel human, stay context-aware, and respond with care. Instead of relying on a single prompt-response loop, I built Sakha around a modular architecture where each component has a clear responsibility.</p>
<p>Here’s a quick overview of the main building blocks:</p>
<ul>
<li><strong>Conversation Processor</strong> – The entry point for all incoming user messages. It handles preprocessing of the <code>user_input</code> and streams it through the conversation graph.</li>
<li><strong>Conversation Graph (LangGraph)</strong> – A dynamic flow engine that guides conversations using stateful, branching logic. This is where Sakha’s adaptability comes from.</li>
<li><strong>Conversation State</strong> – A session-unique state object passed through the entire graph, allowing context to persist and evolve throughout the interaction.</li>
<li><strong>Supervisor</strong> – A smart dispatcher that decides <em>what kind</em> of conversation Sakha should engage in—whether it’s a normal chat, an activity suggestion, or even a crisis intervention.</li>
<li><strong>Crisis Handler</strong> – A specialized module triggered when signs of emotional distress are detected. It provides professional, 24/7 toll-free helpline numbers that users can reach out to.</li>
<li><strong>Chat Engine</strong> – The actual brain behind the conversations, powered by the LLM and dynamically generated prompts based on context and flow.</li>
<li><strong>User, Checkpoint, and Memory Managers</strong> – These handle persistent storage: tracking user info, conversation history, and flow progress across sessions.</li>
<li><strong>Response Templates</strong> – Modular prompt templates that ensure Sakha’s tone stays consistent and compassionate, while allowing for personalization based on user data.</li>
</ul>
<p>Each of these pieces plays a role in making Sakha feel less like a machine—and more like a friend who listens, remembers, and responds with care.</p>
<hr>
</section>
<section id="a-day-in-the-life-of-a-message" class="level2">
<h2 class="anchored" data-anchor-id="a-day-in-the-life-of-a-message">A Day in the Life of a Message</h2>
<p>So, what actually happens when you type “I’m feeling off today” into Sakha?</p>
<p>While the reply might seem instant and effortless, under the hood, a lot of thought goes into how that message is received, processed, and responded to—with care, context, and relevance.</p>
<p>Here’s how a single message flows through Sakha’s system:</p>
<ol type="1">
<li><p><strong>UI → Server</strong><br>
The message begins its journey in the lightweight frontend—a simple HTML/JavaScript interface. When you hit send, the text is forwarded to the backend via a websocket of Flask server, which acts as the bridge between the user and Sakha’s internal logic.</p></li>
<li><p><strong>Flask Endpoint → Conversation Processor</strong><br>
Once the message reaches the server, it’s handed off to the Conversation Processor—the orchestrator of the system. It performs preprocessing and streams the message into the conversation graph, ultimately receiving a response that’s passed back to the server.</p>
<p>If a follow-up or reminder is due, Sakha’s scheduler triggers dedicated Flask endpoints (<code>/reminder</code>, <code>/followup</code>) which initiate their flows independently.</p>
<p>For new sessions, the server also initializes a fresh Conversation Graph and Conversation State, which are then passed to the processor for handling.</p></li>
<li><p><strong>Conversation Processor → Conversation Graph (LangGraph)</strong><br>
The Conversation Processor injects the input into the Conversation Graph, powered by LangGraph. This graph consists of modular nodes and conditional transitions, allowing Sakha to intelligently route the conversation depending on user intent and state.</p></li>
<li><p><strong>Supervisor Node → Chat Flow Decision</strong><br>
Early in the graph, the <strong>Supervisor</strong> node evaluates the input and context. It determines whether this message should enter a general chat flow, trigger an activity suggestion, or—if necessary—be flagged for the <strong>Crisis Handler</strong>.</p></li>
<li><p><strong>Flow Execution → Chat Engine</strong><br>
Once the appropriate flow is selected, the relevant nodes are executed. These call into the <strong>Chat Engine</strong>, which builds the final LLM prompt using conversation state and sends it to the model (e.g., LLaMA3.3 via Groq API). The response is parsed and formatted.</p></li>
<li><p><strong>Server → UI</strong><br>
Finally, the crafted response is returned through the websocket and rendered in the chat UI. To the user, it feels like a natural, thoughtful message—just from a friend who’s really listening.</p></li>
</ol>
<hr>
<p>This flow isn’t just functional—it’s designed for empathy. Each step is intentional, focused on making the interaction feel personal, seamless, and emotionally aware.</p>
<hr>
</section>
<section id="zooming-in-how-sakha-handles-every-message" class="level2">
<h2 class="anchored" data-anchor-id="zooming-in-how-sakha-handles-every-message">Zooming In: How Sakha Handles Every Message</h2>
<p>Let’s take a closer look at what happens <strong>after</strong> a message enters Sakha’s system—how it decides what kind of response to generate, which flow to trigger, and how it tracks everything in between.</p>
<section id="supervisor-making-the-first-decision" class="level3">
<h3 class="anchored" data-anchor-id="supervisor-making-the-first-decision"><strong>Supervisor: Making the First Decision</strong></h3>
<p>Every message first arrives at the <strong>Supervisor</strong> node within the LangGraph-powered conversation graph. This is an LLM-based decision point that examines both the latest user input and the ongoing <strong>conversation state</strong> to determine the appropriate direction for the chat.</p>
<p>Here’s what the Supervisor might decide: - <strong>Crisis Detected</strong> → The flow is routed to the <strong>Crisis Handler</strong>, which shares 24/7 toll-free helpline numbers, ensuring the user has access to professional support. - <strong>No Crisis</strong> → The message is routed to the <strong>Chat Engine</strong>, along with a flow directive:<br>
<code>normal_chat</code>, <code>activity_suggestion</code>, <code>reminder</code>, or <code>follow_up</code>.</p>
<hr>
</section>
<section id="chat-engine-generating-context-aware-responses" class="level3">
<h3 class="anchored" data-anchor-id="chat-engine-generating-context-aware-responses"><strong>Chat Engine: Generating Context-Aware Responses</strong></h3>
<p>The <strong>Chat Engine</strong> is where Sakha’s intelligence comes to life. It’s responsible for crafting thoughtful, structured responses by leveraging multiple subcomponents:</p>
<ol type="1">
<li><p><strong>Summarizer</strong><br>
If the current context is too long for the LLM window, the summarizer condenses it and stores the summary in the <strong>conversation state</strong>—preserving emotional and contextual continuity.</p></li>
<li><p><strong>Chat Flow Manager</strong><br>
Based on the Supervisor’s directive, this module retrieves the appropriate <strong>chat flow object</strong>, which contains prompts and logic for one of the following flows:</p>
<h4 id="current-chat-flows" class="anchored">Current Chat Flows:</h4>
<ul>
<li><p><strong><code>normal_chat</code></strong>: A general conversation mode where Sakha checks in on the user’s well-being. If signs of a low mood are detected, the Supervisor may switch the flow to <code>activity_suggestion</code>.</p></li>
<li><p><strong><code>activity_suggestion</code></strong>: Here, Sakha suggests mood-boosting activities tailored to the user’s emotional state and preferences. A key feature is a <strong>RAG (Retrieval-Augmented Generation)</strong> module using Qdrant to find past activities the user engaged in under similar circumstances (indexed by a situation embedding). If the user agrees, Sakha sets a reminder. If not, it may shift the flow back to <code>normal_chat</code>.</p></li>
<li><p><strong><code>reminder</code></strong>: Triggered externally by <strong>APScheduler</strong> when a reminder is due. Sakha gently motivates the user to carry out the scheduled activity. If the user declines or changes their mind, the Supervisor can redirect to <code>normal_chat</code>.</p></li>
<li><p><strong><code>follow_up</code></strong>: Also triggered by <strong>APScheduler</strong>, but after the activity window has passed. Sakha asks for feedback, which is stored in memory using Qdrant and appended to the activity log.</p></li>
</ul></li>
<li><p><strong>Response Manager</strong><br>
Each flow returns a <strong>structured response</strong> using Sakha’s consistent, modular <strong>Response Templates</strong>. The Response Manager handles these to:</p>
<ul>
<li>Store reminders and feedback in the appropriate database via the <strong>Reminder Manager</strong>.</li>
<li>Log activity outcomes and mood feedback in long-term memory.</li>
<li>Format the response for final delivery.</li>
</ul></li>
</ol>
<p>Finally, once the response is generated and sent back, the <strong>conversation history</strong> and any relevant updates are written back to the <strong>conversation state</strong>, ready for the next interaction.</p>
</section>
</section>
<section id="making-sakha-personal-memory-preferences-and-context" class="level2">
<h2 class="anchored" data-anchor-id="making-sakha-personal-memory-preferences-and-context">Making Sakha Personal: Memory, Preferences, and Context</h2>
<p>One of Sakha’s core goals is to feel <em>personal</em>—not just in tone, but in how it remembers and responds based on your past experiences. While it’s still early in development, the foundation for a memory-aware system is already in place.</p>
<section id="activity-preferences-and-session-context" class="level3">
<h3 class="anchored" data-anchor-id="activity-preferences-and-session-context">Activity Preferences and Session Context</h3>
<p>When a user first registers, their <strong>activity preferences</strong> are collected and stored in a Postgres database. These preferences—like enjoying walks, listening to music, or journaling—are retrieved every time a new conversation begins and injected into the <strong>conversation state</strong>, allowing Sakha to suggest relevant activities right from the start.</p>
</section>
<section id="structured-summarization-for-long-conversations" class="level3">
<h3 class="anchored" data-anchor-id="structured-summarization-for-long-conversations">Structured Summarization for Long Conversations</h3>
<p>To manage lengthy conversations and keep the LLM within token limits, Sakha uses a summarizer component after every 10 exchanges to generate summary. Rather than generating freeform summaries, Sakha uses a structured template to ensure consistency and easy updating:</p>
<p><strong>Summary Format:</strong></p>
<pre><code>1. Main Topic:
2. Key Things the User Said:
3. Key Things Sakha Said or Did:
4. Decisions or Plans Made:
5. Unresolved Topics or Follow-Ups:</code></pre>
<p>This summary is stored in the conversation state and passed across nodes during the conversation flow, allowing Sakha to remain context-aware even when the raw message history grows too large.</p>
</section>
<section id="memories-through-feedback" class="level3">
<h3 class="anchored" data-anchor-id="memories-through-feedback">Memories Through Feedback</h3>
<p>After each activity, Sakha checks in through the <strong>follow-up flow</strong> to ask how it went. The responses—whether the activity was completed, enjoyed, skipped, or why—are stored as <strong>memories</strong> using <strong>Qdrant</strong>, a vector store optimized for similarity search.</p>
<p>To make these memories searchable and context-aware, Sakha uses the <code>BAAI/bge-small-en-v1.5</code> model to convert the user’s situation into an <strong>embedding vector</strong>, which becomes the index key for Qdrant. The stored memory includes:</p>
<ul>
<li><code>user_id</code>, <code>thread_id</code>, <code>activity_id</code></li>
<li><code>user_situation</code>: the emotional or contextual input from the user</li>
<li><code>activity_name</code>, <code>duration</code></li>
<li>Whether the activity was <strong>completed</strong></li>
<li>A <strong>score of enjoyment</strong></li>
<li>Any <strong>reason for skipping</strong></li>
<li>A <code>timestamp</code> of when the feedback was submitted</li>
</ul>
<p>This structure enables Sakha to later suggest activities that have worked in similar emotional contexts, not just random options.</p>
</section>
<section id="whats-missing-today" class="level3">
<h3 class="anchored" data-anchor-id="whats-missing-today">What’s Missing Today</h3>
<p>At this stage, Sakha doesn’t support <em>conversational memory</em> across sessions. That means users can’t yet say things like “Remember what we did last week?” or “Let’s do the walk again,” and expect Sakha to recall past chats or actions explicitly.</p>
<p>Currently, memory is limited to: - In-session context - Activity preferences - Feedback-driven memories for activity suggestions</p>
<p>But this limitation is by design. The goal is to first get structured memory and recall right—ensuring that Sakha offers helpful, accurate nudges based on feedback—before expanding into long-term conversational memory.</p>
<hr>
</section>
</section>
<section id="testing-sakha-what-were-evaluating-and-how" class="level2">
<h2 class="anchored" data-anchor-id="testing-sakha-what-were-evaluating-and-how">Testing Sakha: What We’re Evaluating and How</h2>
<p>Before Sakha is released more broadly, it’s essential to understand not just whether it works—but <em>how well</em> it supports, listens, and responds to real users. The current plan is to manually test interactions with a mix of real users and AI-generated simulated conversations, with a human evaluator scoring each chat based on the following <strong>rubric</strong>.</p>
<p>This rubric is designed to balance emotional intelligence, usability, personalization, and safety—everything that makes Sakha feel like more than a script.</p>
<hr>
<section id="sakha-chatbot-evaluation-rubric" class="level3">
<h3 class="anchored" data-anchor-id="sakha-chatbot-evaluation-rubric">🌱 <strong>Sakha Chatbot Evaluation Rubric</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 7%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Criteria</strong></th>
<th><strong>Rating (1–5)</strong></th>
<th><strong>What We’re Looking For</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Emotional Intelligence &amp; Encouragement</strong></td>
<td></td>
<td>Does Sakha express empathy, validate the user’s feelings, and offer uplifting, compassionate responses without being preachy or robotic?</td>
</tr>
<tr class="even">
<td><strong>Tone Balance (Including Humor)</strong></td>
<td></td>
<td>Is the tone warm, kind, and emotionally appropriate? Is humor used carefully to lighten mood without undermining the situation?</td>
</tr>
<tr class="odd">
<td><strong>Clarity &amp; Simplicity of Responses</strong></td>
<td></td>
<td>Are the responses easy to understand, jargon-free, and structured clearly for the user’s mental/emotional state?</td>
</tr>
<tr class="even">
<td><strong>Personalization &amp; Contextual Awareness</strong></td>
<td></td>
<td>Does Sakha adapt to user preferences, past feedback, or the current mood? Does it feel like a <em>personal</em> assistant, not just a generic chatbot?</td>
</tr>
<tr class="odd">
<td><strong>Memory Usage (Preferences &amp; Feedback Recall)</strong></td>
<td></td>
<td>Is previously stored data (e.g., activity preferences, feedback on past activities) reflected appropriately in recommendations or tone?</td>
</tr>
<tr class="even">
<td><strong>Adaptability to User Mood &amp; Engagement</strong></td>
<td></td>
<td>Can Sakha shift gears—be it from light conversation to deeper support or vice versa—based on mood signals or disengagement?</td>
</tr>
<tr class="odd">
<td><strong>Privacy &amp; Boundary Respect</strong></td>
<td></td>
<td>Does Sakha avoid prying questions, handle sensitive topics gracefully, and make the user feel emotionally safe throughout the interaction?</td>
</tr>
<tr class="even">
<td><strong>Encouragement Toward Self-care or Action</strong></td>
<td></td>
<td>Are nudges toward activity or self-care natural, timely, and well-aligned with user readiness or preferences?</td>
</tr>
<tr class="odd">
<td><strong>Crisis Sensitivity &amp; Handling</strong></td>
<td></td>
<td>Does Sakha detect possible signs of distress and route the user appropriately (e.g., to a crisis handler)? Does it avoid triggering or insensitive responses?</td>
</tr>
<tr class="even">
<td><strong>Response Consistency &amp; Identity</strong></td>
<td></td>
<td>Is Sakha’s tone, values, and personality consistent throughout conversations—even across sessions?</td>
</tr>
<tr class="odd">
<td><strong>Overall Flow &amp; Conversation Coherence</strong></td>
<td></td>
<td>Do the messages feel like part of a thoughtful, coherent interaction rather than disjointed replies?</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="overall-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="overall-evaluation">🧾 <strong>Overall Evaluation</strong></h3>
<ul>
<li><p><strong>Strengths</strong>:<br>
<em>(What aspects of Sakha’s performance were particularly impressive or emotionally resonant?)</em></p></li>
<li><p><strong>Areas for Improvement</strong>:<br>
<em>(Where did it fall short? Any awkwardness, missed cues, or technical slips?)</em></p></li>
<li><p><strong>Final Rating</strong>:<br>
<em>(Sum of all individual scores. Used for comparative tracking as testing progresses.)</em></p></li>
</ul>
<hr>
<p>This rubric will evolve with usage. Early testing will be fully manual, with notes and reflections gathered from test users and evaluators. In the long run, the plan is to partially automate this process using LLMs as scoring agents on synthetic data.</p>
<p>Here’s a suggestion for the <strong>final section</strong> to wrap up the blog with forward-looking momentum and a reflective close:</p>
<hr>
</section>
</section>
<section id="whats-next-closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="whats-next-closing-thoughts">What’s Next &amp; Closing Thoughts</h2>
<p>Sakha started as an idea—to build a chatbot that doesn’t just reply, but <em>connects</em>. From managing structured flows to handling complex emotional contexts, every piece of Sakha was designed with empathy and adaptability in mind. But this is only the beginning.</p>
<section id="whats-next" class="level3">
<h3 class="anchored" data-anchor-id="whats-next">What’s Next:</h3>
<ul>
<li><strong>User Testing &amp; Iteration</strong>: With the core in place, the next step is structured testing using the evaluation rubric above. This feedback loop will guide refinement—both in how Sakha speaks and how it reasons.</li>
<li><strong>Deeper Memory &amp; Personalization</strong>: One limitation today is the lack of long-term memory in natural conversation. The next version will allow users to refer to past chats, explore past activity feedback, and reflect over time.</li>
<li><strong>Multimodal and Voice Interfaces</strong>: Conversations don’t always have to be typed. Voice input and lightweight emotion detection are on the roadmap for future iterations.</li>
<li><strong>Open Source + Deployment</strong>: Sakha will likely be open-sourced with clear documentation, so others can build on it or adapt it to their own domains—whether mental health, coaching, or companionship.</li>
</ul>
<hr>
<p>At its heart, Sakha is a human-first system—less about flashy features, more about thoughtful design. It’s far from perfect, but it’s trying. And sometimes, that’s exactly what we need: something (or someone) that listens, remembers, and tries to help, gently.</p>
<p>Thanks for reading. 💙</p>
<hr>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>chat-bot</category>
  <category>LLM</category>
  <guid>https://rs-saran.github.io/projects/20250122_serene_solace_sakha/</guid>
  <pubDate>Sat, 03 May 2025 18:30:00 GMT</pubDate>
  <media:content url="https://rs-saran.github.io/projects/20250122_serene_solace_sakha/sakha_logo_nobg.png" medium="image" type="image/png" height="93" width="144"/>
</item>
<item>
  <title>Serene Solace - Sophy</title>
  <link>https://rs-saran.github.io/projects/20250105_serene_solace_sophy/</link>
  <description><![CDATA[ 





<p><a href="https://github.com/rs-saran/serene-solace-sophy" class="btn" target="_blank"><i class="fa-brands fa-github" aria-label="github"></i> source code</a> <a href="https://github.com/rs-saran/serene-solace-sophy/blob/main/example_chat.txt" class="btn" target="_blank"><i class="fa-solid fa-bars" aria-label="bars"></i> demo chat</a> <br></p>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>In today’s fast-paced world, mental health is more important than ever. Yet, many people struggle with their emotional well-being, often facing issues alone. Stress, anxiety, and overwhelming thoughts can build up without a clear outlet, leaving individuals feeling isolated and unheard. Fortunately, there’s an old but powerful piece of advice: “<em>Talking to a friend about your problems can help.</em>” When we share our struggles, it often helps to sort through them, gain new perspectives, and release the tension that builds up over time.</p>
<p>It’s important to note that talking to someone is not a cure-all for mental health problems. If you are struggling with a serious mental health condition, it is important to seek professional help from a therapist or counselor. However, talking to someone you trust can be a helpful first step in managing your mental health.</p>
<p>But not everyone has someone they can turn to when they need it most. This gap presents a significant challenge. How can we provide people with a safe, understanding space to express their feelings and navigate their mental health struggles? - Serene Solace</p>
</section>
<section id="scope" class="level3">
<h3 class="anchored" data-anchor-id="scope">Scope</h3>
<ol type="1">
<li>Provides emotional support through empathetic, personalized conversations.</li>
<li>Maintains user confidentiality.</li>
<li>Guides user to Crisis Management helpline when required.</li>
<li>Answers company (serene solace) related questions.</li>
</ol>
</section>
<section id="model-selection" class="level3">
<h3 class="anchored" data-anchor-id="model-selection">Model Selection</h3>
<p>When deciding on the right approach to model selection for Serene Solace, two key factors must be carefully considered: whether to build a model from scratch or use pretrained models, and the financial implications of selecting an API or self-hosted solution.</p>
<ol type="1">
<li><p><strong>Build from Scratch vs.&nbsp;Use Pretrained Models</strong>: Building a model from the ground up requires significant investment in terms of time, expertise, and computational resources. This process is costly and can delay deployment. On the other hand, using pretrained models offers a practical and cost-effective solution. These models have already been trained on large datasets and are ready for fine-tuning, saving both time and resources while providing high-quality performance from the start.</p></li>
<li><p><strong>API Costs vs.&nbsp;Self-Hosting</strong>: When considering the use of language models, the decision to use APIs versus hosting a model locally comes down to ongoing costs, control, and privacy. Using an API service can incur substantial recurring costs, especially as the volume of users and requests grows. These costs can add up over time, potentially making API usage less cost-effective for long-term scalability. Additionally, APIs often come with limitations on data privacy, as user data may be processed by third-party services.</p></li>
</ol>
<p>On the other hand, self-hosting an open-source model allows for full control over data privacy and confidentiality, which is crucial for an emotional support service. By hosting the model locally, we can ensure that sensitive information remains within the organization’s secure infrastructure, addressing privacy concerns. A viable and cost-effective option for self-hosting is the Llama 3.2:1b model, an open-source, lightweight language model. It offers a balance of performance and efficiency, reducing the need for expensive cloud API services while still providing high-quality, empathetic conversations.</p>
<p>In conclusion, for Serene Solace’s focus on providing confidential, emotionally supportive conversations, using an open-source model like Llama 3.2:1b and hosting it locally proves to be both cost-effective and privacy-conscious. This solution minimizes the financial burden of API usage while ensuring user data stays secure.</p>
</section>
<section id="workflow" class="level3">
<h3 class="anchored" data-anchor-id="workflow">Workflow</h3>
<p><img src="https://rs-saran.github.io/projects/20250105_serene_solace_sophy/sophy_state_graph.png" class="img-fluid"></p>
<ol type="1">
<li><strong>State Graph</strong>:</li>
</ol>
<ul>
<li>Sophy uses <code>langgraph</code> to build a state graph.</li>
<li>The graph manages transitions between nodes: <code>Agent</code>, <code>Sophy</code>, <code>CompanyQA</code>, and <code>CrisisHandler</code>.</li>
</ul>
<ol start="2" type="1">
<li><strong>Agent Node</strong>:</li>
</ol>
<ul>
<li>Determines the appropriate route based on user input:</li>
<li><code>Sophy</code>: Normal conversation for mental health support.</li>
<li><code>CrisisHandler</code>: Directs the user to an active crisis helpline.</li>
<li><code>CompanyQA</code>: Answers company-related queries.</li>
</ul>
<ol start="3" type="1">
<li><strong>Sophy</strong>:</li>
</ol>
<ul>
<li>Uses llama3.21b model to chat empathetically.</li>
<li>Dynamically summarizes ongoing exchanges to maintain context and improve conversation flow.</li>
</ul>
<ol start="4" type="1">
<li><strong>Company Q&amp;A</strong>:</li>
</ol>
<ul>
<li>Leverages FAISS for document retrieval and answers user questions based on the <code>company.md</code> file.</li>
</ul>
<ol start="5" type="1">
<li><strong>Crisis Handling</strong>:</li>
</ol>
<ul>
<li>Provides a helpline number if harmful intent is detected.</li>
</ul>
</section>
<section id="future-improvements" class="level3">
<h3 class="anchored" data-anchor-id="future-improvements">Future Improvements</h3>
<ul>
<li>Smoother integration between flows.</li>
<li>UI for the chat.</li>
<li>Expand the knowledge base for company Q&amp;A.</li>
<li>Enhance the detection mechanism for harmful intent.</li>
<li>Multi User Interactions.</li>
</ul>


</section>

 ]]></description>
  <category>NLP</category>
  <category>chat-bot</category>
  <category>RAG</category>
  <category>LLM</category>
  <guid>https://rs-saran.github.io/projects/20250105_serene_solace_sophy/</guid>
  <pubDate>Sat, 04 Jan 2025 18:30:00 GMT</pubDate>
  <media:content url="https://rs-saran.github.io/projects/20250105_serene_solace_sophy/serene_solace_logo_nobg.png" medium="image" type="image/png" height="94" width="144"/>
</item>
<item>
  <title>MO-ME-NTS</title>
  <link>https://rs-saran.github.io/projects/20241201_me_in_moments/</link>
  <description><![CDATA[ 





<p><a href="https://meinmoments.streamlit.app/" class="btn" target="_blank"><i class="fa-brands fa-web-awesome" aria-label="web-awesome"></i> streamlit app</a> <a href="https://github.com/rs-saran/me-in-moments" class="btn" target="_blank"><i class="fa-brands fa-github" aria-label="github"></i> source code</a></p>
<p><br></p>
<!-- # Motivation -->
<section id="introduction-and-problem-statement" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-problem-statement"><strong>Introduction and Problem Statement</strong></h3>
<p>Events are moments of joy, connection, and memories captured through countless photos snapped across multiple devices. However, the joy of reminiscing these moments often turns into a tedious task—finding the photos where you are present amidst a sea of images.</p>
<p>This is where <em>Me in Moments</em> comes in. Designed for event-goers, <em>Me in Moments</em> simplifies the process by allowing users to upload a reference image of themselves. The app then scans the entire collection of event photos to find the ones they appear in, providing a fast and accurate solution to a common problem.</p>
</section>
<section id="features-and-functionalities" class="level3">
<h3 class="anchored" data-anchor-id="features-and-functionalities"><strong>Features and Functionalities</strong></h3>
<p><em>Me in Moments</em> offers a suite of features tailored to enhance the photo discovery experience for event-goers:</p>
<ol type="1">
<li><strong>Facial Similarity Search</strong>: Upload a reference photo and all the event photos, and the app scans through the event’s photo collection to find matching photos.<br>
</li>
<li><strong>Adjustable Similarity Threshold</strong>: Fine-tune the sensitivity of the facial matching algorithm to get results that align with your preferences.<br>
</li>
<li><strong>Download Matches</strong>: Save time by downloading all identified photos with just one click.<br>
</li>
<li><strong>Friendly Interface</strong>: Built with Streamlit, the app delivers an intuitive and interactive experience, making it accessible for users of all technical backgrounds.</li>
</ol>
</section>
<section id="tech-stack" class="level3">
<h3 class="anchored" data-anchor-id="tech-stack"><strong>Tech Stack</strong></h3>
<p><em>Me in Moments</em> utilizes a combination of robust tools and frameworks to deliver accurate facial recognition and a seamless user experience:</p>
<section id="deepface-retinaface-facenet512" class="level4">
<h4 class="anchored" data-anchor-id="deepface-retinaface-facenet512"><strong>1. DeepFace (RetinaFace, FaceNet512)</strong></h4>
<p>DeepFace is a powerful library for facial recognition, providing access to advanced models like <strong>RetinaFace</strong> and <strong>FaceNet512</strong>.</p>
<ul>
<li><strong>RetinaFace</strong>: Excels at detecting faces under diverse conditions, including varied angles and lighting, ensuring reliable face detection across event photos.<br>
</li>
<li><strong>FaceNet512</strong>: Generates precise facial embeddings—unique numerical representations of faces—enabling accurate and efficient comparison of a reference image to event photos.<br>
</li>
<li><strong>Why DeepFace?</strong> The library simplifies the implementation of state-of-the-art facial recognition models, achieving high accuracy without requiring extensive expertise in machine learning.</li>
</ul>
</section>
<section id="opencv-for-image-processing" class="level4">
<h4 class="anchored" data-anchor-id="opencv-for-image-processing"><strong>2. OpenCV (For Image Processing)</strong></h4>
<p>OpenCV is a versatile library for image processing, ensuring the input data is optimized for DeepFace’s recognition models.</p>
<ul>
<li><strong>Image Preprocessing</strong>: Handles essential tasks like resizing, cropping, and adjusting image quality to improve recognition accuracy.<br>
</li>
<li><strong>Real-Time Processing</strong>: Enables efficient handling of large photo collections, ensuring the app performs well even with significant workloads.<br>
</li>
<li><strong>Integration</strong>: Works seamlessly with DeepFace and Streamlit, ensuring smooth workflows from image handling to recognition and display.</li>
</ul>
</section>
<section id="streamlit-for-mvp-deployment" class="level4">
<h4 class="anchored" data-anchor-id="streamlit-for-mvp-deployment"><strong>3. Streamlit (For MVP Deployment)</strong></h4>
<p>Streamlit is an ideal framework for rapid MVP (Minimum Viable Product) development, offering speed and simplicity without sacrificing user interactivity.</p>
<ul>
<li><strong>Speed &amp; Simplicity</strong>: Streamlit allows for quick prototyping with minimal coding, letting developers focus on core features like facial recognition.<br>
</li>
<li><strong>Interactive UI</strong>: Built-in components such as file uploads, sliders, and buttons ensure an intuitive experience. Users can upload a reference photo, adjust the similarity threshold, and download results effortlessly.<br>
</li>
<li><strong>Seamless Deployment</strong>: Streamlit’s straightforward deployment process makes it easy to share the app online without complex setup, making it perfect for launching MVPs.<br>
</li>
<li><strong>Why Streamlit?</strong> While frameworks like Flask or Django offer more extensive capabilities, Streamlit provides a faster and simpler solution for building data-driven apps, especially during the MVP stage.</li>
</ul>
</section>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation"><strong>Implementation</strong></h3>
<section id="workflow" class="level4">
<h4 class="anchored" data-anchor-id="workflow"><strong>Workflow</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rs-saran.github.io/projects/20241201_me_in_moments/workflow-diagram.png" class="img-fluid figure-img"></p>
<figcaption>workflow-diagram</figcaption>
</figure>
</div>
<ol type="1">
<li><p><strong>Image Input</strong><br>
Users upload a <strong>reference image</strong> and a collection of <strong>target images</strong>.</p></li>
<li><p><strong>Image Processing</strong><br>
The <code>ImageProcessor</code> class processes each image using OpenCV to generate multiple enhanced versions:</p>
<ul>
<li><strong>Gray</strong>: Grayscale representation.<br>
</li>
<li><strong>Gray_he</strong>: Grayscale with histogram equalization.<br>
</li>
<li><strong>YUV_he</strong>: YUV color space with histogram equalization.</li>
</ul></li>
<li><p><strong>Face Embedding and Similarity Computation</strong><br>
The <code>FaceEmbeddingService</code> plays a central role by:</p>
<ul>
<li>Detecting faces in the processed images using DeepFace.<br>
</li>
<li>Generating embeddings (numerical representations) for each detected face.<br>
</li>
<li>Providing a similarity function to compute <strong>cosine distance</strong> between embeddings.</li>
</ul></li>
<li><p><strong>Reference Embeddings</strong><br>
The app processes the reference image through the <code>ImageProcessor</code> and <code>FaceEmbeddingService</code>, storing embeddings for all enhanced versions.</p></li>
<li><p><strong>Target Image Comparison</strong><br>
For each target image:</p>
<ul>
<li>The <code>ImageProcessor</code> generates processed versions.<br>
</li>
<li>The <code>FaceEmbeddingService</code> computes embeddings for detected faces.<br>
</li>
<li>Each target embedding is compared against the reference embeddings, and the minimum similarity score is recorded.</li>
</ul></li>
<li><p><strong>Caching and Display</strong></p>
<ul>
<li>All similarity scores are cached for efficient retrieval.<br>
</li>
<li>A slider (default threshold: <strong>0.4</strong>) allows users to adjust matching sensitivity.<br>
</li>
<li>Images with similarity scores below the threshold are displayed, helping users identify relevant photos.</li>
</ul></li>
</ol>
</section>
</section>
<section id="challenges-and-solutions" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-solutions"><strong>Challenges and Solutions</strong></h3>
<p>During the development of <em>Me in Moments</em>, one significant challenge was ensuring accurate facial similarity comparisons, especially when users were photographed in different lighting conditions. Simple direct comparisons between images were not providing reliable results, as lighting variations often affected the facial features.</p>
<section id="solution" class="level4">
<h4 class="anchored" data-anchor-id="solution"><strong>Solution</strong></h4>
<p>To address this, I implemented a preprocessing step using OpenCV to generate multiple versions of the images before passing them to the facial recognition model:<br>
- <strong>Gray</strong>: A standard grayscale version of the image.<br>
- <strong>Gray_he</strong>: A grayscale version with histogram equalization to improve contrast and brightness.<br>
- <strong>YUV_he</strong>: YUV color space with histogram equalization for better feature extraction.</p>
<p>These preprocessing techniques enhanced the accuracy of facial detection and similarity comparison, allowing <em>Me in Moments</em> to perform better under varying lighting conditions.</p>
</section>
</section>
<section id="future-enhancements" class="level3">
<h3 class="anchored" data-anchor-id="future-enhancements"><strong>Future Enhancements</strong></h3>
<p>The current version of <em>Me in Moments</em> provides a quick and accurate facial similarity search for event photos. Moving forward, several key improvements are planned to enhance scalability, performance, and user experience:</p>
<section id="scalability" class="level4">
<h4 class="anchored" data-anchor-id="scalability"><strong>Scalability</strong></h4>
<p>To better handle larger datasets and improve efficiency, the following improvements are in the pipeline:<br>
- <strong>Centralized Storage</strong>: Users should be able to upload photos from multiple devices to a centralized storage system, making it easier to manage large collections of event photos.<br>
- <strong>Parallel Processing</strong>: Images could be processed in parallel as soon as they are uploaded, significantly speeding up the face recognition and embedding generation process.<br>
- <strong>Vector Database</strong>: Face embeddings could be stored in a vector database, enabling fast and scalable similarity searches, even across large datasets.<br>
- <strong>Unified User Interface</strong>: The UI will be enhanced to allow users to upload reference images and search for matches across the entire event’s photo collection.</p>
<!-- #TODO performance section
#TODO MVP workflow diagram -->
<p><br></p>


</section>
</section>

 ]]></description>
  <category>Computer Vision</category>
  <guid>https://rs-saran.github.io/projects/20241201_me_in_moments/</guid>
  <pubDate>Sat, 30 Nov 2024 18:30:00 GMT</pubDate>
  <media:content url="https://rs-saran.github.io/projects/20241201_me_in_moments/me_in_moments_logo_nobg.png" medium="image" type="image/png" height="95" width="144"/>
</item>
</channel>
</rss>
